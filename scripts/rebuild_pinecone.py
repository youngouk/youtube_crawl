#!/usr/bin/env python3
"""
Pinecone ë°ì´í„° ì¬êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸

R2ì—ì„œ ì›ë³¸ ë°ì´í„°ë¥¼ ì½ì–´ ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆë¡œ Pineconeì— ì¬ì—…ë¡œë“œí•©ë‹ˆë‹¤.
- ì²­í‚¹: 120í† í°, ì˜¤ë²„ë© 20
- ID í˜•ì‹: {video_id}_chunk_{n}
- í•„ë“œëª…: chunk_text, start_time, end_time (êµ¬ë²„ì „)
- ë©”íƒ€ë°ì´í„°: specialty, credentials ë“± (ì‹ ë²„ì „ ì¶”ê°€)

ì‚¬ìš©ë²•:
    python scripts/rebuild_pinecone.py              # ì „ì²´ ì‹¤í–‰
    python scripts/rebuild_pinecone.py --resume     # ì¤‘ë‹¨ ì§€ì ë¶€í„° ì¬ê°œ
    python scripts/rebuild_pinecone.py --dry-run    # í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ì €ì¥ ì•ˆí•¨)
"""

import os
import json
import time
import logging
import argparse
from datetime import datetime
from pathlib import Path
from typing import Any

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì‹¤ì œ ì‹¤í–‰ ì‹œ í•„ìš”)
# boto3, pinecone, tiktokenì€ Task 2ì—ì„œ ì‚¬ìš©
try:
    import boto3
    from botocore.config import Config
    from pinecone import Pinecone
    import tiktoken
except ImportError as e:
    # --help ì‹¤í–‰ ì‹œì—ëŠ” í•„ìš” ì—†ìŒ
    pass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ (ì‹¤ì œ ì‹¤í–‰ ì‹œ í•„ìš”)
try:
    from config.settings import settings
    from src.processors.gemini_processor import GeminiProcessor
except ImportError:
    # --help ì‹¤í–‰ ì‹œì—ëŠ” í•„ìš” ì—†ìŒ
    pass

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# ìƒìˆ˜
CHUNK_SIZE = 120          # í† í°
CHUNK_OVERLAP = 20        # í† í°
PINECONE_BATCH_SIZE = 100
PROGRESS_FILE = "data/rebuild_progress.json"


def log(msg: str = "") -> None:
    """ì‹¤ì‹œê°„ ë¡œê·¸ ì¶œë ¥"""
    print(msg, flush=True)


class ProgressTracker:
    """ì§„í–‰ ìƒíƒœ ì¶”ì  (ì¤‘ë‹¨/ì¬ê°œ ì§€ì›)"""

    def __init__(self, progress_file: str = PROGRESS_FILE):
        self.progress_file = Path(progress_file)
        self.data = self._load()

    def _load(self) -> dict:
        """ì§„í–‰ ìƒíƒœ íŒŒì¼ ë¡œë“œ"""
        if self.progress_file.exists():
            with open(self.progress_file, 'r') as f:
                return json.load(f)
        return {
            "completed_videos": [],
            "failed_videos": [],
            "total_chunks": 0,
            "started_at": None,
            "last_updated": None
        }

    def save(self) -> None:
        """ì§„í–‰ ìƒíƒœ ì €ì¥"""
        self.data["last_updated"] = datetime.now().isoformat()
        self.progress_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.progress_file, 'w') as f:
            json.dump(self.data, f, indent=2, ensure_ascii=False)

    def is_completed(self, video_id: str) -> bool:
        """ë¹„ë””ì˜¤ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return video_id in self.data["completed_videos"]

    def mark_completed(self, video_id: str, chunk_count: int) -> None:
        """ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ"""
        if video_id not in self.data["completed_videos"]:
            self.data["completed_videos"].append(video_id)
            self.data["total_chunks"] += chunk_count
        self.save()

    def mark_failed(self, video_id: str, error: str) -> None:
        """ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨ í‘œì‹œ"""
        self.data["failed_videos"].append({"video_id": video_id, "error": error})
        self.save()

    def start(self) -> None:
        """ì‘ì—… ì‹œì‘ ì‹œê°„ ê¸°ë¡"""
        if not self.data["started_at"]:
            self.data["started_at"] = datetime.now().isoformat()
        self.save()

    def reset(self) -> None:
        """ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”"""
        self.data = {
            "completed_videos": [],
            "failed_videos": [],
            "total_chunks": 0,
            "started_at": datetime.now().isoformat(),
            "last_updated": None
        }
        self.save()

    def get_summary(self) -> dict:
        """ì§„í–‰ ìƒíƒœ ìš”ì•½ ë°˜í™˜"""
        return {
            "completed_count": len(self.data["completed_videos"]),
            "failed_count": len(self.data["failed_videos"]),
            "total_chunks": self.data["total_chunks"],
            "started_at": self.data["started_at"],
            "last_updated": self.data["last_updated"]
        }


class R2Client:
    """R2 ìŠ¤í† ë¦¬ì§€ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.client = boto3.client(
            's3',
            endpoint_url=settings.R2_ENDPOINT_URL,
            aws_access_key_id=settings.R2_ACCESS_KEY_ID,
            aws_secret_access_key=settings.R2_SECRET_ACCESS_KEY,
            config=Config(signature_version='s3v4'),
            region_name='auto'
        )
        self.bucket = settings.R2_BUCKET_NAME

    def get_json(self, key: str) -> dict | list | None:
        """R2ì—ì„œ JSON íŒŒì¼ ì½ê¸°"""
        try:
            response = self.client.get_object(Bucket=self.bucket, Key=key)
            return json.loads(response['Body'].read().decode('utf-8'))
        except Exception as e:
            logger.debug(f"R2 ì½ê¸° ì‹¤íŒ¨ ({key}): {e}")
            return None

    def list_video_ids(self) -> list[str]:
        """transcripts/ í´ë”ì˜ ëª¨ë“  ë¹„ë””ì˜¤ ID ëª©ë¡ ë°˜í™˜"""
        video_ids = set()
        paginator = self.client.get_paginator('list_objects_v2')

        for page in paginator.paginate(Bucket=self.bucket, Prefix='transcripts/'):
            for obj in page.get('Contents', []):
                parts = obj['Key'].split('/')
                if len(parts) >= 2 and parts[1]:
                    video_ids.add(parts[1])

        return sorted(list(video_ids))

    def get_video_data(self, video_id: str) -> dict | None:
        """ë¹„ë””ì˜¤ì˜ raw.jsonê³¼ refined.json ë¡œë“œ"""
        raw = self.get_json(f"transcripts/{video_id}/raw.json")
        refined = self.get_json(f"transcripts/{video_id}/refined.json")

        if not raw or not refined:
            return None

        return {
            "video_id": video_id,
            "raw_segments": raw,
            "refined_text": refined.get("text", "")
        }


def load_channels_metadata() -> dict[str, dict]:
    """channels.jsonì—ì„œ ì±„ë„ë³„ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
    channels_path = Path("data/channels.json")
    if not channels_path.exists():
        logger.error("data/channels.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        return {}

    with open(channels_path, 'r', encoding='utf-8') as f:
        channels = json.load(f)

    # channel_idë¥¼ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    return {ch['channel_id']: ch for ch in channels}


# tiktoken ì¸ì½”ë” (ì§€ì—° ì´ˆê¸°í™”)
_TOKENIZER = None


def get_tokenizer():
    """í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì§€ì—° ì´ˆê¸°í™”)"""
    global _TOKENIZER
    if _TOKENIZER is None:
        try:
            _TOKENIZER = tiktoken.get_encoding("cl100k_base")
        except Exception:
            _TOKENIZER = tiktoken.get_encoding("gpt2")
    return _TOKENIZER


def count_tokens(text: str) -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ë°˜í™˜"""
    return len(get_tokenizer().encode(text))


def chunk_with_timestamps(
    refined_text: str,
    raw_segments: list[dict],
    chunk_size: int = CHUNK_SIZE,
    overlap: int = CHUNK_OVERLAP
) -> list[dict]:
    """
    refined_textë¥¼ ì²­í‚¹í•˜ê³  raw_segmentsì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤í•‘

    Args:
        refined_text: ì •ì œëœ ì „ì²´ í…ìŠ¤íŠ¸
        raw_segments: ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ [{"text": "...", "start": 0.0, "duration": 5.0}, ...]
        chunk_size: ì²­í¬ í¬ê¸° (í† í°)
        overlap: ì˜¤ë²„ë© í¬ê¸° (í† í°)

    Returns:
        [{"text": "...", "start_time": 0.0, "end_time": 30.5, "chunk_index": 0}, ...]
    """
    if not refined_text or not raw_segments:
        return []

    # 1. raw_segmentsì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤í•‘ ìƒì„±
    # ë¬¸ì ìœ„ì¹˜ -> ì‹œê°„ ë§¤í•‘ì„ ìœ„í•œ ëˆ„ì  í…ìŠ¤íŠ¸ êµ¬ì¶•
    char_to_time = []
    current_pos = 0

    for seg in raw_segments:
        seg_text = seg.get('text', '')
        start = seg.get('start', 0)
        duration = seg.get('duration', 0)
        end = start + duration

        # ê° ë¬¸ìì— ì‹œê°„ í• ë‹¹ (ì„ í˜• ë³´ê°„)
        for i, char in enumerate(seg_text):
            ratio = i / max(len(seg_text), 1)
            time_at_char = start + (duration * ratio)
            char_to_time.append(time_at_char)

        # ì„¸ê·¸ë¨¼íŠ¸ ì‚¬ì´ ê³µë°±
        char_to_time.append(end)

    # 2. refined_textë¥¼ í† í° ë‹¨ìœ„ë¡œ ì²­í‚¹
    tokenizer = get_tokenizer()
    tokens = tokenizer.encode(refined_text)
    chunks = []

    start_idx = 0
    while start_idx < len(tokens):
        end_idx = min(start_idx + chunk_size, len(tokens))
        chunk_tokens = tokens[start_idx:end_idx]
        chunk_text = tokenizer.decode(chunk_tokens)

        # 3. ì²­í¬ì˜ ì‹œì‘/ì¢…ë£Œ ì‹œê°„ ì¶”ì •
        # refined_textì—ì„œ ì²­í¬ì˜ ëŒ€ëµì ì¸ ìœ„ì¹˜ ê³„ì‚°
        text_start_ratio = start_idx / max(len(tokens), 1)
        text_end_ratio = end_idx / max(len(tokens), 1)

        # ì „ì²´ ì‹œê°„ ë²”ìœ„
        if raw_segments:
            total_duration = raw_segments[-1].get('start', 0) + raw_segments[-1].get('duration', 0)
        else:
            total_duration = 0

        start_time = total_duration * text_start_ratio
        end_time = total_duration * text_end_ratio

        chunks.append({
            "text": chunk_text.strip(),
            "start_time": round(start_time, 2),
            "end_time": round(end_time, 2),
            "chunk_index": len(chunks)
        })

        # ë‹¤ìŒ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© ì ìš©)
        start_idx += (chunk_size - overlap)

    return chunks


def generate_context_and_topics(
    processor: GeminiProcessor,
    chunk_text: str,
    video_title: str,
    channel_name: str
) -> tuple[str, list[str]]:
    """
    Gemini APIë¡œ ì²­í¬ì˜ contextì™€ topics ìƒì„±

    Args:
        processor: GeminiProcessor ì¸ìŠ¤í„´ìŠ¤
        chunk_text: ì²­í¬ í…ìŠ¤íŠ¸
        video_title: ë¹„ë””ì˜¤ ì œëª©
        channel_name: ì±„ë„ ì´ë¦„

    Returns:
        (context, topics)
    """
    # ë¹„ë””ì˜¤ ìš”ì•½ ìƒì„± (ê°„ë‹¨ ë²„ì „)
    video_summary = f"'{video_title}' - {channel_name} ì±„ë„ì˜ ì˜ë£Œ/ê±´ê°• ì •ë³´ ì˜ìƒ"

    try:
        # GeminiProcessorì˜ ê¸°ì¡´ ë©”ì„œë“œ í™œìš©
        context, topics = processor.generate_chunk_context_and_topics(
            chunk_text, video_summary
        )

        # topicsê°€ ì—†ê±°ë‚˜ ë¹ˆ ê²½ìš° ê¸°ë³¸ê°’
        if not topics or topics == ['NONE']:
            topics = ['NONE']

        return context, topics

    except Exception as e:
        logger.warning(f"Context/Topics ìƒì„± ì‹¤íŒ¨: {e}")
        # í´ë°±: ê¸°ë³¸ context ìƒì„±
        context = f"ì´ ë‚´ìš©ì€ {channel_name} ì±„ë„ì˜ '{video_title}' ì˜ìƒì—ì„œ ë°œì·Œí•œ ê²ƒì…ë‹ˆë‹¤."
        return context, ['NONE']


class RateLimiter:
    """API í˜¸ì¶œ ì†ë„ ì œí•œ"""

    def __init__(self, calls_per_minute: int = 15):
        self.calls_per_minute = calls_per_minute
        self.interval = 60.0 / calls_per_minute
        self.last_call = 0

    def wait(self) -> None:
        """í•„ìš”ì‹œ ëŒ€ê¸°"""
        now = time.time()
        elapsed = now - self.last_call
        if elapsed < self.interval:
            time.sleep(self.interval - elapsed)
        self.last_call = time.time()


def create_pinecone_vector(
    video_id: str,
    chunk_index: int,
    chunk_text: str,
    context: str,
    topics: list[str],
    start_time: float,
    end_time: float,
    video_title: str,
    channel_id: str,
    channel_name: str,
    channel_meta: dict,
    embedding: list[float],
    published_at: str = ""
) -> dict:
    """
    Pinecone ì €ì¥ìš© ë²¡í„° êµ¬ì¡° ìƒì„±

    ìµœì¢… ìŠ¤í‚¤ë§ˆ:
    - ID: {video_id}_chunk_{n}
    - êµ¬ë²„ì „ í•„ë“œëª…: chunk_text, start_time, end_time
    - ì‹ ë²„ì „ ë©”íƒ€ë°ì´í„°: specialty, credentials ë“±
    """
    # specialty ì •ê·œí™”
    specialty = channel_meta.get('specialty', [])
    if isinstance(specialty, list):
        specialty = specialty[0] if specialty else 'NONE'

    return {
        'id': f"{video_id}_chunk_{chunk_index}",
        'values': embedding,
        'metadata': {
            # êµ¬ë²„ì „ í•„ë“œëª…
            'chunk_text': chunk_text,
            'start_time': start_time,
            'end_time': end_time,

            # ê³µí†µ í•„ë“œ
            'video_id': video_id,
            'video_title': video_title,
            'channel_id': channel_id,
            'channel_name': channel_name,
            'chunk_index': chunk_index,
            'context': context,
            'topics': topics,
            'processed_at': datetime.now().isoformat(),

            # ì‹ ë²„ì „ ì¶”ê°€ í•„ë“œ
            'is_verified_professional': channel_meta.get('is_verified_professional', False),
            'specialty': specialty,
            'credentials': channel_meta.get('credentials', ''),
            'video_url': f'https://www.youtube.com/watch?v={video_id}',
            'source_type': 'youtube',
            'published_at': published_at
        }
    }


def generate_embedding(
    processor: GeminiProcessor,
    chunk_text: str,
    context: str,
    rate_limiter: RateLimiter
) -> list[float] | None:
    """
    ì²­í¬ì˜ ì„ë² ë”© ë²¡í„° ìƒì„±

    context + chunk_textë¥¼ ê²°í•©í•˜ì—¬ ì„ë² ë”©
    """
    rate_limiter.wait()

    try:
        combined_text = f"{context}\n\n{chunk_text}"
        embedding = processor.get_embedding(combined_text)
        return embedding
    except Exception as e:
        logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return None


def delete_all_vectors(index) -> int:
    """Pinecone ì¸ë±ìŠ¤ì˜ ëª¨ë“  ë²¡í„° ì‚­ì œ"""
    try:
        # ì „ì²´ ì‚­ì œ
        index.delete(delete_all=True)
        log("  ëª¨ë“  ë²¡í„° ì‚­ì œ ì™„ë£Œ")
        return 0
    except Exception as e:
        logger.error(f"ë²¡í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise


def upsert_vectors(index, vectors: list[dict]) -> int:
    """ë²¡í„° ë°°ì¹˜ upsert"""
    if not vectors:
        return 0

    try:
        for i in range(0, len(vectors), PINECONE_BATCH_SIZE):
            batch = vectors[i:i + PINECONE_BATCH_SIZE]
            index.upsert(vectors=batch)
        return len(vectors)
    except Exception as e:
        logger.error(f"ë²¡í„° upsert ì‹¤íŒ¨: {e}")
        raise


def process_video(
    video_id: str,
    r2: R2Client,
    processor: GeminiProcessor,
    channels_meta: dict,
    rate_limiter: RateLimiter,
    dry_run: bool = False
) -> list[dict]:
    """
    ë‹¨ì¼ ë¹„ë””ì˜¤ ì²˜ë¦¬: R2 ë¡œë“œ â†’ ì²­í‚¹ â†’ Context/Topics â†’ ì„ë² ë”© â†’ ë²¡í„° ìƒì„±

    Returns:
        ìƒì„±ëœ ë²¡í„° ë¦¬ìŠ¤íŠ¸
    """
    # 1. R2ì—ì„œ ë°ì´í„° ë¡œë“œ
    data = r2.get_video_data(video_id)
    if not data:
        raise ValueError(f"R2 ë°ì´í„° ì—†ìŒ: {video_id}")

    # 2. ë©”íƒ€ë°ì´í„° ì¡°íšŒ (R2ì—ì„œ ì¶”ê°€ ì •ë³´ ë¡œë“œ)
    metadata = r2.get_json(f"metadata/{video_id}/metadata.json") or {}
    video_title = metadata.get('video_title', f'Video {video_id}')
    channel_id = metadata.get('channel_id', '')
    channel_name = metadata.get('channel_name', 'Unknown')
    published_at = metadata.get('published_at', '')

    # ì±„ë„ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
    channel_meta = channels_meta.get(channel_id, {})
    if not channel_meta:
        # channel_nameìœ¼ë¡œ ì¬ì‹œë„
        for ch in channels_meta.values():
            if ch.get('name') == channel_name:
                channel_meta = ch
                break

    # 3. ì²­í‚¹
    chunks = chunk_with_timestamps(
        data['refined_text'],
        data['raw_segments']
    )

    if not chunks:
        raise ValueError(f"ì²­í¬ ìƒì„± ì‹¤íŒ¨: {video_id}")

    # 4. ê° ì²­í¬ ì²˜ë¦¬
    vectors = []
    for chunk in chunks:
        # Context & Topics ìƒì„±
        rate_limiter.wait()
        context, topics = generate_context_and_topics(
            processor,
            chunk['text'],
            video_title,
            channel_name
        )

        if dry_run:
            # dry-run ëª¨ë“œì—ì„œëŠ” ë”ë¯¸ ì„ë² ë”© ì‚¬ìš©
            embedding = [0.0] * 1024
        else:
            # ì„ë² ë”© ìƒì„±
            embedding = generate_embedding(
                processor,
                chunk['text'],
                context,
                rate_limiter
            )

            if not embedding:
                logger.warning(f"ì„ë² ë”© ì‹¤íŒ¨, ìŠ¤í‚µ: {video_id}_chunk_{chunk['chunk_index']}")
                continue

        # ë²¡í„° êµ¬ì¡° ìƒì„±
        vector = create_pinecone_vector(
            video_id=video_id,
            chunk_index=chunk['chunk_index'],
            chunk_text=chunk['text'],
            context=context,
            topics=topics,
            start_time=chunk['start_time'],
            end_time=chunk['end_time'],
            video_title=video_title,
            channel_id=channel_id,
            channel_name=channel_name,
            channel_meta=channel_meta,
            embedding=embedding,
            published_at=published_at
        )

        vectors.append(vector)

    return vectors


def main():
    parser = argparse.ArgumentParser(description='Pinecone ë°ì´í„° ì¬êµ¬ì¶•')
    parser.add_argument('--resume', action='store_true', help='ì¤‘ë‹¨ ì§€ì ë¶€í„° ì¬ê°œ')
    parser.add_argument('--dry-run', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì‹¤ì œ ì €ì¥ ì•ˆí•¨)')
    parser.add_argument('--limit', type=int, default=0, help='ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ìˆ˜ ì œí•œ (0=ì „ì²´)')
    parser.add_argument('--skip-delete', action='store_true', help='ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ìŠ¤í‚µ')
    args = parser.parse_args()

    log("=" * 60)
    log("Pinecone ë°ì´í„° ì¬êµ¬ì¶•")
    log("=" * 60)
    log(f"ì²­í¬ í¬ê¸°: {CHUNK_SIZE}í† í°, ì˜¤ë²„ë©: {CHUNK_OVERLAP}í† í°")
    log(f"ëª¨ë“œ: {'DRY-RUN (í…ŒìŠ¤íŠ¸)' if args.dry_run else 'ì‹¤ì œ ì‹¤í–‰'}")
    log()

    # ì´ˆê¸°í™”
    progress = ProgressTracker()
    if not args.resume:
        log("âš ï¸ ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”")
        progress.reset()
    progress.start()

    r2 = R2Client()
    channels_meta = load_channels_metadata()
    processor = GeminiProcessor()
    rate_limiter = RateLimiter(calls_per_minute=15)

    # Pinecone ì´ˆê¸°í™”
    pc = Pinecone(api_key=settings.PINECONE_API_KEY)
    index = pc.Index(settings.PINECONE_INDEX_NAME)

    log(f"ì±„ë„ ë©”íƒ€ë°ì´í„°: {len(channels_meta)}ê°œ")
    log(f"Pinecone ì¸ë±ìŠ¤: {settings.PINECONE_INDEX_NAME}")

    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
    if not args.resume and not args.skip_delete and not args.dry_run:
        log("\nğŸ—‘ï¸ ê¸°ì¡´ Pinecone ë°ì´í„° ì‚­ì œ ì¤‘...")
        stats = index.describe_index_stats()
        log(f"  ì‚­ì œ ì „ ë²¡í„° ìˆ˜: {stats.total_vector_count:,}")
        delete_all_vectors(index)
        time.sleep(2)  # ì‚­ì œ ë°˜ì˜ ëŒ€ê¸°

    # ë¹„ë””ì˜¤ ëª©ë¡ ì¡°íšŒ
    video_ids = r2.list_video_ids()
    log(f"\nR2 ë¹„ë””ì˜¤ ìˆ˜: {len(video_ids)}ê°œ")

    # ì´ë¯¸ ì™„ë£Œëœ ë¹„ë””ì˜¤ ì œì™¸
    if args.resume:
        before = len(video_ids)
        video_ids = [v for v in video_ids if not progress.is_completed(v)]
        log(f"ì™„ë£Œëœ ë¹„ë””ì˜¤ ì œì™¸: {before} â†’ {len(video_ids)}ê°œ")

    # ì œí•œ ì ìš©
    if args.limit > 0:
        video_ids = video_ids[:args.limit]
        log(f"ì œí•œ ì ìš©: {len(video_ids)}ê°œ")

    # ì²˜ë¦¬ ì‹œì‘
    log("\n" + "=" * 60)
    log("ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘")
    log("=" * 60)

    total = len(video_ids)
    success = 0
    failed = 0
    total_chunks = 0

    for i, video_id in enumerate(video_ids, 1):
        try:
            log(f"\n[{i}/{total}] {video_id}")

            # ë¹„ë””ì˜¤ ì²˜ë¦¬
            vectors = process_video(
                video_id=video_id,
                r2=r2,
                processor=processor,
                channels_meta=channels_meta,
                rate_limiter=rate_limiter,
                dry_run=args.dry_run
            )

            log(f"  ì²­í¬ ìˆ˜: {len(vectors)}ê°œ")

            # Pinecone ì €ì¥
            if not args.dry_run and vectors:
                upsert_vectors(index, vectors)
                log(f"  âœ… Pinecone ì €ì¥ ì™„ë£Œ")

            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            progress.mark_completed(video_id, len(vectors))
            success += 1
            total_chunks += len(vectors)

        except Exception as e:
            logger.error(f"  âŒ ì‹¤íŒ¨: {e}")
            progress.mark_failed(video_id, str(e))
            failed += 1

    # ìµœì¢… í†µê³„
    log("\n" + "=" * 60)
    log("ì™„ë£Œ!")
    log("=" * 60)
    log(f"ì„±ê³µ: {success}ê°œ ë¹„ë””ì˜¤")
    log(f"ì‹¤íŒ¨: {failed}ê°œ ë¹„ë””ì˜¤")
    log(f"ì´ ì²­í¬: {total_chunks}ê°œ")

    if not args.dry_run:
        stats = index.describe_index_stats()
        log(f"Pinecone ìµœì¢… ë²¡í„° ìˆ˜: {stats.total_vector_count:,}")


if __name__ == "__main__":
    main()
