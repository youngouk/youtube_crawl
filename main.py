"""
Medical RAG íŒŒì´í”„ë¼ì¸ ë©”ì¸ ì‹¤í–‰ ëª¨ë“ˆì…ë‹ˆë‹¤.

ì†Œì•„ê³¼ ê´€ë ¨ YouTube ì±„ë„ë“¤ì„ ìˆœíšŒí•˜ë©° ë¹„ë””ì˜¤ë¥¼ ìˆ˜ì§‘í•˜ê³ ,
íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ë¥¼ ì •ì œí•œ í›„ Pineconeì— ì¸ë±ì‹±í•©ë‹ˆë‹¤.
"""
import json
import time
import argparse
from datetime import datetime, timezone
from typing import Any
from config.settings import settings
from src.collectors.youtube_collector import YouTubeCollector
from src.collectors.transcript_collector import TranscriptCollector
from src.processors.gemini_processor import GeminiProcessor
from src.processors.chunker import Chunker
from src.storage.r2_storage import get_storage
from src.storage.state_manager import StateManager
from src.storage.channel_state_manager import ChannelStateManager
from src.vector_db.pinecone_manager import PineconeManager
from src.utils.logger import logger


def create_chunk_metadata(
    chunk_text: str,
    context: str,
    video_id: str,
    video_title: str,
    channel_id: str,
    channel_name: str,
    chunk_index: int,
    published_at: str,
    is_verified_professional: bool,
    specialty: str,
    credentials: str,
    timestamp_start: str = '',
    timestamp_end: str = '',
    topics: list[str] | None = None
) -> dict[str, Any]:
    """
    Pinecone ì €ì¥ìš© ì²­í¬ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        chunk_text: ì²­í¬ í…ìŠ¤íŠ¸
        context: Contextual Retrievalë¡œ ìƒì„±ëœ ì»¨í…ìŠ¤íŠ¸
        video_id: YouTube ë¹„ë””ì˜¤ ID
        video_title: ë¹„ë””ì˜¤ ì œëª©
        channel_id: YouTube ì±„ë„ ID
        channel_name: ì±„ë„ ì´ë¦„
        chunk_index: ì²­í¬ ì¸ë±ìŠ¤
        published_at: ë¹„ë””ì˜¤ ê²Œì‹œì¼ (ISO 8601)
        is_verified_professional: ì˜ë£Œ ì „ë¬¸ê°€ ì¸ì¦ ì—¬ë¶€
        specialty: ì „ë¬¸ ë¶„ì•¼ (ì˜ˆ: ì†Œì•„ê³¼)
        credentials: ì „ë¬¸ê°€ ìê²© ì •ë³´ (ì˜ˆ: ì†Œì•„ì²­ì†Œë…„ê³¼ ì „ë¬¸ì˜)
        timestamp_start: ì²­í¬ ì‹œì‘ ì‹œê°„ (ì˜ˆ: "02:30")
        timestamp_end: ì²­í¬ ì¢…ë£Œ ì‹œê°„ (ì˜ˆ: "04:15")
        topics: ì˜ë£Œ ê´€ë ¨ í† í”½ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸

    Returns:
        ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    return {
        'text': chunk_text,
        'context': context,
        'video_id': video_id,
        'video_title': video_title,
        'channel_id': channel_id,
        'channel_name': channel_name,
        'chunk_index': chunk_index,
        'source_type': 'youtube',
        'video_url': f'https://www.youtube.com/watch?v={video_id}',
        'published_at': published_at,
        'is_verified_professional': is_verified_professional,
        'specialty': specialty,
        'credentials': credentials,
        'timestamp_start': timestamp_start,
        'timestamp_end': timestamp_end,
        'topics': topics or [],
        'processed_at': datetime.now(timezone.utc).isoformat()
    }


def format_transcript_to_text(transcript_list: list[dict[str, Any]]) -> str:
    """Converts transcript list to a single string."""
    return " ".join([t['text'] for t in transcript_list])


def format_timestamp(seconds: float) -> str:
    """
    ì´ˆ ë‹¨ìœ„ ì‹œê°„ì„ MM:SS í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        seconds: ì´ˆ ë‹¨ìœ„ ì‹œê°„ (ì˜ˆ: 150.5)

    Returns:
        MM:SS í˜•ì‹ ë¬¸ìì—´ (ì˜ˆ: "02:30")
    """
    total_seconds = int(seconds)
    minutes = total_seconds // 60
    secs = total_seconds % 60
    return f"{minutes:02d}:{secs:02d}"


def normalize_specialty(specialty_value: Any) -> str:
    """
    specialty í•„ë“œë¥¼ ë¬¸ìì—´ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.

    channels.jsonì—ì„œ ë°°ì—´ ë˜ëŠ” ë¬¸ìì—´ë¡œ ì €ì¥ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
    ì¼ê´€ëœ ë¬¸ìì—´ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        specialty_value: ë°°ì—´ ë˜ëŠ” ë¬¸ìì—´ í˜•íƒœì˜ specialty ê°’

    Returns:
        ì •ê·œí™”ëœ ë¬¸ìì—´ (ë°°ì—´ì¸ ê²½ìš° ì²« ë²ˆì§¸ ê°’)
    """
    if isinstance(specialty_value, list):
        return specialty_value[0] if specialty_value else ''
    return str(specialty_value) if specialty_value else ''

def main(
    retry_failed: bool = False,
    skip_completed_channels: bool = True,
    specific_channel: str | None = None,
    max_results: int = 50,
    sort_override: str | None = None
) -> None:
    """
    íŒŒì´í”„ë¼ì¸ ë©”ì¸ ì‹¤í–‰ ë£¨í”„ì…ë‹ˆë‹¤.
    """
    logger.info("=" * 60)
    logger.info("Starting Medical RAG Pipeline - Multi-Channel Processing")
    logger.info("=" * 60)
    
    if retry_failed:
        logger.info("Retry mode enabled: will retry failed videos")
    if specific_channel:
        logger.info(f"Processing specific channel only: {specific_channel}")
    
    # ì •ë ¬ ë° ê°œìˆ˜ ì„¤ì • ë¡œê¹…
    current_sort = sort_override or settings.VIDEO_SORT_BY
    logger.info(f"Sort mode: {current_sort}, Max results: {max_results}")

    # Initialize components
    storage = get_storage()
    state_manager = StateManager()
    channel_state_manager = ChannelStateManager()
    
    # Check needed keys
    if not settings.YOUTUBE_API_KEY:
        logger.error("YOUTUBE_API_KEY missing. Exiting.")
        return
        
    yt_collector = YouTubeCollector()
    transcript_collector = TranscriptCollector()
    
    try:
        gemini = GeminiProcessor()
    except Exception as e:
        logger.error(f"Failed to init Gemini: {e}")
        return

    # Pinecone is optional for initial collection/processing test
    pinecone_manager = None
    if settings.PINECONE_API_KEY:
        pinecone_manager = PineconeManager()
        
    chunker = Chunker(chunk_size=120, chunk_overlap=20)

    # Load Channels
    channels_path = "channels.json"
    if not storage.exists(channels_path):
         # Try local fallback if not found in storage (for dev)
         local_channels = settings.LOCAL_DATA_DIR / "channels.json"
         if local_channels.exists():
             with open(local_channels, 'r') as f:
                 channels = json.load(f)
         else:
             logger.error("channels.json not found.")
             return
    else:
        channels = storage.load_json(channels_path)

    # ì±„ë„ í•„í„°ë§ (íŠ¹ì • ì±„ë„ë§Œ ì²˜ë¦¬í•˜ê±°ë‚˜ ì „ì²´ ì²˜ë¦¬)
    channels_to_process = channels
    if specific_channel:
        channels_to_process = [c for c in channels if c['channel_id'] == specific_channel]
        if not channels_to_process:
            logger.error(f"Channel not found: {specific_channel}")
            return

    total_channels = len(channels_to_process)
    processed_channels = 0

    for channel_idx, channel in enumerate(channels_to_process, 1):
        channel_id = channel['channel_id']
        channel_name = channel['name']

        logger.info("")
        logger.info("=" * 60)
        logger.info(f"[ì±„ë„ {channel_idx}/{total_channels}] {channel_name}")
        logger.info(f"ì±„ë„ ID: {channel_id}")
        logger.info("=" * 60)

        # ì´ë¯¸ ì™„ë£Œëœ ì±„ë„ ìŠ¤í‚µ
        if skip_completed_channels and channel_state_manager.is_channel_completed(channel_id):
            logger.info(f"âœ… ì´ë¯¸ ì™„ë£Œëœ ì±„ë„ì…ë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            processed_channels += 1
            continue

        # 1. Fetch Videos
        sort_by = sort_override or settings.VIDEO_SORT_BY
        
        if sort_by == "both":
            logger.info(f"Fetching both 'recent' and 'views' (max {max_results} each)")
            recent_vids = yt_collector.get_channel_videos_sorted(
                channel_id,
                max_results=max_results,
                sort_by="recent"
            )
            popular_vids = yt_collector.get_channel_videos_sorted(
                channel_id,
                max_results=max_results,
                sort_by="views",
                fetch_pool=settings.VIDEO_FETCH_POOL
            )
            # Merge and deduplicate
            video_dict = {v['video_id']: v for v in (recent_vids + popular_vids)}
            videos = list(video_dict.values())
            logger.info(f"Combined videos: {len(recent_vids)} recent + {len(popular_vids)} views -> {len(videos)} unique")
        else:
            videos = yt_collector.get_channel_videos_sorted(
                channel_id,
                max_results=max_results,
                sort_by=sort_by,
                fetch_pool=settings.VIDEO_FETCH_POOL
            )

        if not videos:
            logger.warning(f"ì±„ë„ì—ì„œ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {channel_name}")
            continue

        # ì±„ë„ ìƒíƒœ ì´ˆê¸°í™”
        channel_state_manager.init_channel(channel_id, channel_name, len(videos))
        logger.info(f"ğŸ“º ì´ {len(videos)}ê°œ ë¹„ë””ì˜¤ ë°œê²¬")

        # Save video list
        storage.save_json(f"raw/videos/{channel_id}/list.json", videos)

        for video_idx, video in enumerate(videos, 1):
            video_id = video['video_id']
            video_title = video['title']

            # Check Status
            status = state_manager.get_video_status(video_id)
            if status:
                video_status = status.get('status')
                if video_status == 'completed':
                    logger.info(f"  [{video_idx}/{len(videos)}] â­ï¸ ìŠ¤í‚µ (ì™„ë£Œë¨): {video_title[:30]}...")
                    channel_state_manager.update_video_result(channel_id, success=True, skipped=True)
                    continue
                elif video_status == 'failed' and not retry_failed:
                    logger.info(f"  [{video_idx}/{len(videos)}] â­ï¸ ìŠ¤í‚µ (ì‹¤íŒ¨): {video_title[:30]}...")
                    channel_state_manager.update_video_result(channel_id, success=False, skipped=True)
                    continue

            logger.info(f"  [{video_idx}/{len(videos)}] ğŸ¬ ì²˜ë¦¬ ì¤‘: {video_title[:40]}...")
            state_manager.update_video_status(video_id, "processing", "transcript_download")
            
            try:
                # 2. Download Transcript (YouTube ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°)
                time.sleep(3)  # ìš”ì²­ ê°„ 3ì´ˆ ëŒ€ê¸°
                transcript = transcript_collector.get_transcript(video_id)
                if not transcript:
                    state_manager.update_video_status(video_id, "failed", error="No transcript")
                    channel_state_manager.update_video_result(
                        channel_id, success=False, error_type="No transcript"
                    )
                    continue
                
                storage.save_json(f"transcripts/{video_id}/raw.json", transcript)
                
                # 3. Refine Transcript
                state_manager.update_video_status(video_id, "processing", "refinement")
                raw_text = format_transcript_to_text(transcript)
                refined_text = gemini.refine_transcript(raw_text)
                
                if not refined_text:
                    logger.warning(f"Refinement failed/empty for {video_id}")
                    state_manager.update_video_status(video_id, "failed", error="Refinement failed")
                    channel_state_manager.update_video_result(
                        channel_id, success=False, error_type="Refinement failed"
                    )
                    continue
                    
                storage.save_json(f"transcripts/{video_id}/refined.json", {"text": refined_text})
                
                # 4. Summarize Video (for context)
                state_manager.update_video_status(video_id, "processing", "summarization")
                video_summary = gemini.summarize_video(refined_text)

                if not video_summary:
                    logger.warning(f"Summary generation failed for {video_id}")
                    state_manager.update_video_status(video_id, "failed", error="Summary failed")
                    channel_state_manager.update_video_result(
                        channel_id, success=False, error_type="Summary failed"
                    )
                    continue

                storage.save_json(f"metadata/{video_id}.json", {
                    **video,
                    "summary": video_summary,
                    "processed_at": time.strftime("%Y-%m-%d")
                })

                # 5. Chunking & Contextual Retrieval (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
                state_manager.update_video_status(video_id, "processing", "chunking")

                # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ìˆëŠ” ì²­í‚¹ ì‚¬ìš©
                chunks_with_timestamps = chunker.split_transcript_with_timestamps(transcript)

                chunk_data_list = []
                vectors_to_upsert = []

                for idx, chunk_info in enumerate(chunks_with_timestamps):
                    chunk_text = chunk_info['text']
                    start_time = chunk_info['start_time']
                    end_time = chunk_info['end_time']

                    # Generate Context and Topics (ë‹¨ì¼ API í˜¸ì¶œë¡œ ìµœì í™”)
                    context, topics = gemini.generate_chunk_context_and_topics(
                        chunk_text, video_summary
                    )

                    # Combine for Embedding
                    final_text_for_embedding = f"{context}\n\n{chunk_text}"

                    # Generate Embedding
                    embedding = gemini.get_embedding(final_text_for_embedding)

                    chunk_meta = {
                        "chunk_index": idx,
                        "text": chunk_text,
                        "context": context,
                        "video_id": video_id,
                        "video_title": video_title,
                        "channel_id": channel_id,
                        "channel_name": channel['name'],
                        "start_time": start_time,
                        "end_time": end_time,
                        "topics": topics
                    }
                    chunk_data_list.append(chunk_meta)

                    if embedding:
                        # í™•ì¥ëœ ë©”íƒ€ë°ì´í„° ìƒì„± (ëª¨ë“  í•„ë“œ í¬í•¨)
                        metadata = create_chunk_metadata(
                            chunk_text=chunk_text,
                            context=context,
                            video_id=video_id,
                            video_title=video_title,
                            channel_id=channel_id,
                            channel_name=channel['name'],
                            chunk_index=idx,
                            published_at=video.get('published_at', ''),
                            is_verified_professional=channel.get('is_verified_professional', False),
                            specialty=normalize_specialty(channel.get('specialty')),
                            credentials=channel.get('credentials', ''),
                            timestamp_start=format_timestamp(start_time),
                            timestamp_end=format_timestamp(end_time),
                            topics=topics
                        )
                        vector = {
                            "id": f"{video_id}_{idx}",
                            "values": embedding,
                            "metadata": metadata
                        }
                        vectors_to_upsert.append(vector)

                storage.save_json(f"chunks/{video_id}/chunks.json", chunk_data_list)

                # 6. Indexing - ì¡°ê±´ë¶€ ì™„ë£Œ ì²˜ë¦¬
                if not vectors_to_upsert:
                    logger.warning(f"No embeddings generated for {video_id}")
                    state_manager.update_video_status(video_id, "failed", error="No embeddings generated")
                    channel_state_manager.update_video_result(
                        channel_id, success=False, error_type="No embeddings"
                    )
                    continue

                if pinecone_manager:
                    state_manager.update_video_status(video_id, "processing", "indexing")
                    pinecone_manager.upsert_vectors(vectors_to_upsert)
                    state_manager.update_video_status(video_id, "completed")
                    channel_state_manager.update_video_result(channel_id, success=True)
                    logger.info(f"    âœ… ì™„ë£Œ: {video_id}")
                else:
                    state_manager.update_video_status(video_id, "processed_no_index")
                    channel_state_manager.update_video_result(channel_id, success=True)
                    logger.info(f"    âœ… ì™„ë£Œ (ì¸ë±ì‹± ì œì™¸): {video_id}")

            except Exception as e:
                logger.error(f"    âŒ ì—ëŸ¬: {video_id} - {e}")
                state_manager.update_video_status(video_id, "failed", error=str(e))
                channel_state_manager.update_video_result(
                    channel_id, success=False, error_type=str(e)[:50]
                )

        # ì±„ë„ ì²˜ë¦¬ ì™„ë£Œ
        channel_state_manager.complete_channel(channel_id)
        processed_channels += 1

        # ì±„ë„ ì²˜ë¦¬ ê²°ê³¼ ì¶œë ¥
        channel_status = channel_state_manager.get_channel_status(channel_id)
        if channel_status:
            logger.info("")
            logger.info(f"ğŸ“Š ì±„ë„ ì²˜ë¦¬ ì™„ë£Œ: {channel_name}")
            logger.info(f"   - ì²˜ë¦¬ë¨: {channel_status.get('processed_videos', 0)}")
            logger.info(f"   - ì‹¤íŒ¨: {channel_status.get('failed_videos', 0)}")
            logger.info(f"   - ìë§‰ì—†ìŒ: {channel_status.get('no_transcript_videos', 0)}")
            logger.info(f"   - ìŠ¤í‚µ: {channel_status.get('skipped_videos', 0)}")

    # ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ ìš”ì•½
    logger.info("")
    logger.info("=" * 60)
    logger.info("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ!")
    logger.info("=" * 60)

    summary = channel_state_manager.get_summary()
    logger.info(f"ğŸ“Š ì „ì²´ ìš”ì•½:")
    logger.info(f"   - ì²˜ë¦¬ëœ ì±„ë„: {summary['completed_channels']}/{summary['total_channels']}")
    logger.info(f"   - ì´ ì²˜ë¦¬ëœ ë¹„ë””ì˜¤: {summary['total_videos_processed']}")
    logger.info(f"   - ì´ ì‹¤íŒ¨ ë¹„ë””ì˜¤: {summary['total_videos_failed']}")
    logger.info(f"   - ìë§‰ ì—†ëŠ” ë¹„ë””ì˜¤: {summary['total_no_transcript']}")


def show_status() -> None:
    """ì±„ë„ë³„ ì²˜ë¦¬ í˜„í™©ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
    channel_state_manager = ChannelStateManager()
    summary = channel_state_manager.get_summary()

    print("\n" + "=" * 70)
    print("ğŸ“Š Medical RAG Pipeline - ì±„ë„ ì²˜ë¦¬ í˜„í™©")
    print("=" * 70)

    print(f"\nğŸ”¢ ì „ì²´ í†µê³„:")
    print(f"   - ì´ ì±„ë„ ìˆ˜: {summary['total_channels']}")
    print(f"   - ì™„ë£Œëœ ì±„ë„: {summary['completed_channels']}")
    print(f"   - ì²˜ë¦¬ ì¤‘ ì±„ë„: {summary['processing_channels']}")
    print(f"   - ì´ ì²˜ë¦¬ëœ ë¹„ë””ì˜¤: {summary['total_videos_processed']}")
    print(f"   - ì´ ì‹¤íŒ¨ ë¹„ë””ì˜¤: {summary['total_videos_failed']}")
    print(f"   - ìë§‰ ì—†ëŠ” ë¹„ë””ì˜¤: {summary['total_no_transcript']}")

    if summary['channels']:
        print(f"\nğŸ“º ì±„ë„ë³„ í˜„í™©:")
        print("-" * 70)
        print(f"{'ì±„ë„ëª…':<30} {'ìƒíƒœ':<12} {'ì§„í–‰ë¥ ':<10} {'ì„±ê³µ':<6} {'ì‹¤íŒ¨':<6} {'ìŠ¤í‚µ':<6}")
        print("-" * 70)

        for ch in summary['channels']:
            status_emoji = "âœ…" if ch['status'] == 'completed' else "ğŸ”„" if ch['status'] == 'processing' else "â¸ï¸"
            name = ch['name'][:28] + ".." if len(ch['name']) > 30 else ch['name']
            print(f"{name:<30} {status_emoji} {ch['status']:<10} {ch['progress']:<10} {ch['processed']:<6} {ch['failed']:<6} {ch['skipped']:<6}")

        print("-" * 70)

    print()


def list_channels() -> None:
    """ë“±ë¡ëœ ì±„ë„ ëª©ë¡ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
    channels_path = settings.LOCAL_DATA_DIR / "channels.json"

    if not channels_path.exists():
        # ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‹œë„
        channels_path = settings.LOCAL_DATA_DIR.parent / "channels.json"

    if not channels_path.exists():
        print("âŒ channels.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    with open(channels_path, 'r', encoding='utf-8') as f:
        channels = json.load(f)

    print("\n" + "=" * 70)
    print("ğŸ“º ë“±ë¡ëœ YouTube ì±„ë„ ëª©ë¡")
    print("=" * 70)

    for idx, ch in enumerate(channels, 1):
        print(f"\n[{idx}] {ch['name']}")
        print(f"    ì±„ë„ ID: {ch['channel_id']}")
        print(f"    ì „ë¬¸ë¶„ì•¼: {', '.join(ch.get('specialty', []))}")
        print(f"    ìê²©: {ch.get('credentials', 'N/A')}")
        print(f"    ì„¤ëª…: {ch.get('description', 'N/A')}")

    print("\n" + "=" * 70)
    print(f"ì´ {len(channels)}ê°œ ì±„ë„ ë“±ë¡ë¨")
    print()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Medical RAG Pipeline - YouTube ì˜ë£Œ ì½˜í…ì¸  ìˆ˜ì§‘ ë° ì¸ë±ì‹±",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python main.py                     # ëª¨ë“  ì±„ë„ ì²˜ë¦¬ (ì™„ë£Œëœ ì±„ë„ ìŠ¤í‚µ)
  python main.py --retry             # ì‹¤íŒ¨í•œ ë¹„ë””ì˜¤ ì¬ì‹œë„
  python main.py --channel UC...     # íŠ¹ì • ì±„ë„ë§Œ ì²˜ë¦¬
  python main.py --status            # ì±„ë„ë³„ ì²˜ë¦¬ í˜„í™© í™•ì¸
  python main.py --list-channels     # ë“±ë¡ëœ ì±„ë„ ëª©ë¡ í™•ì¸
  python main.py --reset-all         # ëª¨ë“  ì±„ë„ ìƒíƒœ ë¦¬ì…‹ (ì£¼ì˜!)
        """
    )

    parser.add_argument(
        '--retry', action='store_true',
        help='ì‹¤íŒ¨í•œ ë¹„ë””ì˜¤ ì¬ì‹œë„'
    )
    parser.add_argument(
        '--channel', type=str, default=None,
        help='íŠ¹ì • ì±„ë„ IDë§Œ ì²˜ë¦¬'
    )
    parser.add_argument(
        '--no-skip', action='store_true',
        help='ì™„ë£Œëœ ì±„ë„ë„ ë‹¤ì‹œ ì²˜ë¦¬'
    )
    parser.add_argument(
        '--status', action='store_true',
        help='ì±„ë„ë³„ ì²˜ë¦¬ í˜„í™© í™•ì¸'
    )
    parser.add_argument(
        '--list-channels', action='store_true',
        help='ë“±ë¡ëœ ì±„ë„ ëª©ë¡ í™•ì¸'
    )
    parser.add_argument(
        '--reset-all', action='store_true',
        help='ëª¨ë“  ì±„ë„ ìƒíƒœ ë¦¬ì…‹ (ì£¼ì˜: ì¬ì²˜ë¦¬ í•„ìš”)'
    )

    parser.add_argument(
        '--max-results', type=int, default=50,
        help='ìˆ˜ì§‘í•  ë¹„ë””ì˜¤ ìˆ˜ (ë‹¨ì¼ ì •ë ¬ ê¸°ì¤€)'
    )
    parser.add_argument(
        '--sort', type=str, default=None,
        choices=['recent', 'views', 'both'],
        help='ë¹„ë””ì˜¤ ì •ë ¬ ê¸°ì¤€ (recent, views, both)'
    )

    args = parser.parse_args()

    if args.status:
        show_status()
    elif args.list_channels:
        list_channels()
    elif args.reset_all:
        confirm = input("âš ï¸ ëª¨ë“  ì±„ë„ ìƒíƒœë¥¼ ë¦¬ì…‹í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): ")
        if confirm.lower() == 'yes':
            import os
            state_file = settings.LOCAL_DATA_DIR / "channel_state.json"
            if state_file.exists():
                os.remove(state_file)
                print("âœ… ì±„ë„ ìƒíƒœê°€ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print("â„¹ï¸ ë¦¬ì…‹í•  ìƒíƒœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        main(
            retry_failed=args.retry,
            skip_completed_channels=not args.no_skip,
            specific_channel=args.channel,
            max_results=args.max_results,
            sort_override=args.sort
        )
